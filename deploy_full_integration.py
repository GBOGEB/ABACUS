#!/usr/bin/env python3
"""
Full Integration Deployment Script
===================================
Version: 1.0.0
Purpose: Deploy and validate complete GBOGEB/ABACUS â†” DOW integration

This script handles:
- Staging environment deployment
- Full integration testing with DOW pipeline
- Convergence metrics validation
- Performance benchmarking
- Production deployment
- Continuous monitoring setup
- Automated alerts configuration
"""

import os
import sys
import json
import time
import logging
import subprocess
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Optional, Tuple

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


class FullIntegrationDeployer:
    """Comprehensive deployment and integration manager"""
    
    def __init__(self):
        self.workspace_root = Path.cwd()
        self.deployment_log = []
        self.start_time = datetime.now()
        self.metrics = {
            "staging_deployed": False,
            "integration_tests_passed": False,
            "convergence_validated": False,
            "performance_benchmarked": False,
            "production_deployed": False,
            "monitoring_configured": False,
            "alerts_configured": False
        }
        
    def run_command(self, command: str, description: str) -> Tuple[bool, str]:
        """Execute command and capture output"""
        logger.info(f"â–¶ {description}")
        
        try:
            result = subprocess.run(
                command,
                shell=True,
                capture_output=True,
                text=True,
                timeout=600
            )
            
            success = result.returncode == 0
            output = result.stdout + result.stderr
            
            if success:
                logger.info(f"âœ“ {description} - SUCCESS")
            else:
                logger.error(f"âœ— {description} - FAILED")
                logger.error(f"Output: {output[:500]}")
            
            self.deployment_log.append({
                "timestamp": datetime.now().isoformat(),
                "description": description,
                "success": success,
                "output": output[:1000]
            })
            
            return success, output
            
        except Exception as e:
            logger.error(f"âœ— {description} - ERROR: {e}")
            return False, str(e)
    
    def step_1_deploy_staging(self) -> bool:
        """Step 1: Deploy to staging environment"""
        logger.info("\n" + "="*80)
        logger.info("STEP 1: DEPLOY TO STAGING ENVIRONMENT")
        logger.info("="*80)
        
        # Create staging directory structure
        staging_dirs = [
            "staging/DOW",
            "staging/DMAIC_V3",
            "staging/INTEGRATED_OUTPUT",
            "staging/logs",
            "staging/config"
        ]
        
        for dir_path in staging_dirs:
            Path(dir_path).mkdir(parents=True, exist_ok=True)
            logger.info(f"âœ“ Created: {dir_path}")
        
        # Copy integration files to staging
        integration_files = [
            "GBOGEB_ABACUS_DOW_INTEGRATION_BRIDGE.py",
            "UNIFIED_GLOB_CONFIG.yaml",
            "test_integration_bridge.py"
        ]
        
        for file in integration_files:
            if Path(file).exists():
                import shutil
                shutil.copy(file, f"staging/{file}")
                logger.info(f"âœ“ Copied to staging: {file}")
        
        # Create staging configuration
        staging_config = {
            "environment": "staging",
            "deployment_time": datetime.now().isoformat(),
            "integration_mode": "unified",
            "iterations": 3,
            "enable_agents": True,
            "enable_convergence": True,
            "enable_monitoring": True
        }
        
        with open("staging/config/staging_config.json", 'w') as f:
            json.dump(staging_config, f, indent=2)
        
        logger.info("âœ“ Staging configuration created")
        
        self.metrics["staging_deployed"] = True
        logger.info("\nâœ… STAGING DEPLOYMENT COMPLETE")
        return True
    
    def step_2_run_full_integration_tests(self) -> bool:
        """Step 2: Run full integration tests with DOW pipeline"""
        logger.info("\n" + "="*80)
        logger.info("STEP 2: RUN FULL INTEGRATION TESTS WITH DOW PIPELINE")
        logger.info("="*80)
        
        test_scenarios = [
            {
                "name": "DOW-only Mode Test",
                "command": 'python -c "from GBOGEB_ABACUS_DOW_INTEGRATION_BRIDGE import GBOGEBAbacusDOWBridge, IntegrationConfig, IntegrationMode; config = IntegrationConfig(mode=IntegrationMode.DOW_ONLY, iterations=2); bridge = GBOGEBAbacusDOWBridge(config=config); results = bridge.execute_integrated_pipeline(); print(f\'Status: {results[\\\"status\\\"]}\'); assert results[\'status\'] in [\'completed\', \'failed\']"'
            },
            {
                "name": "Unified Mode Test",
                "command": 'python -c "from GBOGEB_ABACUS_DOW_INTEGRATION_BRIDGE import GBOGEBAbacusDOWBridge, IntegrationConfig, IntegrationMode; config = IntegrationConfig(mode=IntegrationMode.UNIFIED, iterations=2); bridge = GBOGEBAbacusDOWBridge(config=config); results = bridge.execute_integrated_pipeline(); print(f\'Status: {results[\\\"status\\\"]}\'); print(f\'Duration: {results.get(\\\"duration_seconds\\\", 0):.2f}s\')"'
            },
            {
                "name": "Parallel Mode Test",
                "command": 'python -c "from GBOGEB_ABACUS_DOW_INTEGRATION_BRIDGE import GBOGEBAbacusDOWBridge, IntegrationConfig, IntegrationMode; config = IntegrationConfig(mode=IntegrationMode.PARALLEL, iterations=1); bridge = GBOGEBAbacusDOWBridge(config=config); results = bridge.execute_integrated_pipeline(); print(f\'Status: {results[\\\"status\\\"]}\')"'
            },
            {
                "name": "Sequential Mode Test",
                "command": 'python -c "from GBOGEB_ABACUS_DOW_INTEGRATION_BRIDGE import GBOGEBAbacusDOWBridge, IntegrationConfig, IntegrationMode; config = IntegrationConfig(mode=IntegrationMode.SEQUENTIAL, iterations=1); bridge = GBOGEBAbacusDOWBridge(config=config); results = bridge.execute_integrated_pipeline(); print(f\'Status: {results[\\\"status\\\"]}\')"'
            }
        ]
        
        passed_tests = 0
        total_tests = len(test_scenarios)
        
        for scenario in test_scenarios:
            success, output = self.run_command(
                scenario["command"],
                scenario["name"]
            )
            if success:
                passed_tests += 1
        
        logger.info(f"\nðŸ“Š Integration Tests: {passed_tests}/{total_tests} PASSED")
        
        self.metrics["integration_tests_passed"] = passed_tests >= (total_tests * 0.75)
        
        if self.metrics["integration_tests_passed"]:
            logger.info("âœ… INTEGRATION TESTS PASSED")
            return True
        else:
            logger.warning("âš ï¸ SOME INTEGRATION TESTS FAILED")
            return False
    
    def step_3_validate_convergence(self) -> bool:
        """Step 3: Validate convergence metrics"""
        logger.info("\n" + "="*80)
        logger.info("STEP 3: VALIDATE CONVERGENCE METRICS")
        logger.info("="*80)
        
        # Run convergence validation test
        convergence_test = """
from GBOGEB_ABACUS_DOW_INTEGRATION_BRIDGE import GBOGEBAbacusDOWBridge, IntegrationConfig, IntegrationMode
import json

config = IntegrationConfig(
    mode=IntegrationMode.UNIFIED,
    iterations=3,
    enable_convergence=True
)

bridge = GBOGEBAbacusDOWBridge(config=config)
results = bridge.execute_integrated_pipeline()

# Check convergence
convergence_achieved = bridge.metrics.get('convergence_achieved', False)
print(f'Convergence Achieved: {convergence_achieved}')
print(f'Total Iterations: {config.iterations}')
print(f'Metrics: {json.dumps(bridge.metrics, indent=2)}')

# Validate convergence history
if 'unified_results' in results and results['unified_results']:
    convergence_history = results['unified_results'].get('convergence_history', [])
    print(f'Convergence History Length: {len(convergence_history)}')
    
    if convergence_history:
        latest_convergence = convergence_history[-1]
        print(f'Latest Convergence: {json.dumps(latest_convergence, indent=2)}')
"""
        
        success, output = self.run_command(
            f'python -c "{convergence_test}"',
            "Convergence Validation Test"
        )
        
        # Analyze convergence metrics
        convergence_validated = "Convergence" in output
        
        self.metrics["convergence_validated"] = convergence_validated
        
        if convergence_validated:
            logger.info("âœ… CONVERGENCE METRICS VALIDATED")
            return True
        else:
            logger.warning("âš ï¸ CONVERGENCE VALIDATION INCOMPLETE")
            return True  # Non-blocking
    
    def step_4_performance_benchmark(self) -> bool:
        """Step 4: Review performance benchmarks"""
        logger.info("\n" + "="*80)
        logger.info("STEP 4: PERFORMANCE BENCHMARKING")
        logger.info("="*80)
        
        benchmark_tests = [
            {
                "name": "Quick Execution Benchmark",
                "iterations": 1,
                "target_time": 5.0
            },
            {
                "name": "Standard Execution Benchmark",
                "iterations": 2,
                "target_time": 10.0
            },
            {
                "name": "Full Execution Benchmark",
                "iterations": 3,
                "target_time": 15.0
            }
        ]
        
        benchmark_results = []
        
        for benchmark in benchmark_tests:
            start = time.time()
            
            test_cmd = f"""
from GBOGEB_ABACUS_DOW_INTEGRATION_BRIDGE import GBOGEBAbacusDOWBridge, IntegrationConfig, IntegrationMode
import time

start_time = time.time()
config = IntegrationConfig(mode=IntegrationMode.UNIFIED, iterations={benchmark['iterations']})
bridge = GBOGEBAbacusDOWBridge(config=config)
results = bridge.execute_integrated_pipeline()
duration = time.time() - start_time

print(f'Duration: {{duration:.2f}}s')
print(f'Target: {benchmark['target_time']}s')
print(f'Status: {{results["status"]}}')
"""
            
            success, output = self.run_command(
                f'python -c "{test_cmd}"',
                benchmark["name"]
            )
            
            duration = time.time() - start
            
            benchmark_results.append({
                "name": benchmark["name"],
                "iterations": benchmark["iterations"],
                "duration": duration,
                "target": benchmark["target_time"],
                "passed": duration <= benchmark["target_time"] * 1.5  # 50% tolerance
            })
        
        # Generate benchmark report
        logger.info("\nðŸ“Š Performance Benchmark Results:")
        for result in benchmark_results:
            status = "âœ“" if result["passed"] else "âœ—"
            logger.info(f"  {status} {result['name']}: {result['duration']:.2f}s (target: {result['target']}s)")
        
        passed_benchmarks = sum(1 for r in benchmark_results if r["passed"])
        self.metrics["performance_benchmarked"] = passed_benchmarks >= len(benchmark_results) * 0.66
        
        # Save benchmark report
        with open("PERFORMANCE_BENCHMARK_REPORT.json", 'w') as f:
            json.dump({
                "timestamp": datetime.now().isoformat(),
                "benchmarks": benchmark_results,
                "passed": passed_benchmarks,
                "total": len(benchmark_results),
                "success_rate": passed_benchmarks / len(benchmark_results)
            }, f, indent=2)
        
        logger.info("âœ… PERFORMANCE BENCHMARKING COMPLETE")
        return True
    
    def step_5_deploy_production(self) -> bool:
        """Step 5: Deploy to production"""
        logger.info("\n" + "="*80)
        logger.info("STEP 5: DEPLOY TO PRODUCTION")
        logger.info("="*80)
        
        # Verify all prerequisites
        prerequisites = [
            ("Staging Deployed", self.metrics["staging_deployed"]),
            ("Integration Tests Passed", self.metrics["integration_tests_passed"]),
            ("Convergence Validated", self.metrics["convergence_validated"]),
            ("Performance Benchmarked", self.metrics["performance_benchmarked"])
        ]
        
        logger.info("\nðŸ“‹ Production Deployment Prerequisites:")
        all_met = True
        for name, status in prerequisites:
            icon = "âœ“" if status else "âœ—"
            logger.info(f"  {icon} {name}")
            if not status:
                all_met = False
        
        if not all_met:
            logger.warning("\nâš ï¸ NOT ALL PREREQUISITES MET - SKIPPING PRODUCTION DEPLOYMENT")
            return False
        
        # Create production directory structure
        production_dirs = [
            "production/DOW",
            "production/DMAIC_V3",
            "production/INTEGRATED_OUTPUT",
            "production/logs",
            "production/config",
            "production/monitoring"
        ]
        
        for dir_path in production_dirs:
            Path(dir_path).mkdir(parents=True, exist_ok=True)
            logger.info(f"âœ“ Created: {dir_path}")
        
        # Copy validated files to production
        production_files = [
            "GBOGEB_ABACUS_DOW_INTEGRATION_BRIDGE.py",
            "UNIFIED_GLOB_CONFIG.yaml",
            "INTEGRATION_GUIDE.md"
        ]
        
        for file in production_files:
            if Path(file).exists():
                import shutil
                shutil.copy(file, f"production/{file}")
                logger.info(f"âœ“ Deployed to production: {file}")
        
        # Create production configuration
        production_config = {
            "environment": "production",
            "deployment_time": datetime.now().isoformat(),
            "integration_mode": "unified",
            "iterations": 3,
            "enable_agents": True,
            "enable_convergence": True,
            "enable_monitoring": True,
            "enable_alerts": True,
            "enable_git_commits": True,
            "enable_idempotency": True
        }
        
        with open("production/config/production_config.json", 'w') as f:
            json.dump(production_config, f, indent=2)
        
        logger.info("âœ“ Production configuration created")
        
        self.metrics["production_deployed"] = True
        logger.info("\nâœ… PRODUCTION DEPLOYMENT COMPLETE")
        return True
    
    def step_6_setup_monitoring(self) -> bool:
        """Step 6: Set up continuous monitoring"""
        logger.info("\n" + "="*80)
        logger.info("STEP 6: SET UP CONTINUOUS MONITORING")
        logger.info("="*80)
        
        # Create monitoring configuration
        monitoring_config = {
            "enabled": True,
            "check_interval_seconds": 300,
            "metrics_to_monitor": [
                "execution_success_rate",
                "convergence_rate",
                "average_execution_time",
                "error_rate",
                "artifact_generation_rate"
            ],
            "thresholds": {
                "min_success_rate": 0.80,
                "max_execution_time": 30.0,
                "max_error_rate": 0.20,
                "min_convergence_rate": 0.70
            },
            "alert_channels": [
                "log_file",
                "metrics_file"
            ]
        }
        
        monitoring_dir = Path("production/monitoring")
        monitoring_dir.mkdir(parents=True, exist_ok=True)
        
        with open(monitoring_dir / "monitoring_config.json", 'w') as f:
            json.dump(monitoring_config, f, indent=2)
        
        logger.info("âœ“ Monitoring configuration created")
        
        # Create monitoring script
        monitoring_script = """#!/usr/bin/env python3
import json
import time
from pathlib import Path
from datetime import datetime

def check_metrics():
    metrics_file = Path('production/monitoring/current_metrics.json')
    if metrics_file.exists():
        with open(metrics_file) as f:
            metrics = json.load(f)
        return metrics
    return {}

def log_status(message):
    log_file = Path('production/logs/monitoring.log')
    with open(log_file, 'a') as f:
        f.write(f'{datetime.now().isoformat()} - {message}\\n')

while True:
    metrics = check_metrics()
    log_status(f'Monitoring check: {json.dumps(metrics)}')
    time.sleep(300)  # Check every 5 minutes
"""
        
        with open(monitoring_dir / "monitor.py", 'w') as f:
            f.write(monitoring_script)
        
        logger.info("âœ“ Monitoring script created")
        
        self.metrics["monitoring_configured"] = True
        logger.info("\nâœ… CONTINUOUS MONITORING CONFIGURED")
        return True
    
    def step_7_configure_alerts(self) -> bool:
        """Step 7: Configure automated alerts"""
        logger.info("\n" + "="*80)
        logger.info("STEP 7: CONFIGURE AUTOMATED ALERTS")
        logger.info("="*80)
        
        # Create alerts configuration
        alerts_config = {
            "enabled": True,
            "alert_rules": [
                {
                    "name": "High Error Rate",
                    "condition": "error_rate > 0.20",
                    "severity": "critical",
                    "action": "log_and_notify"
                },
                {
                    "name": "Low Success Rate",
                    "condition": "success_rate < 0.80",
                    "severity": "warning",
                    "action": "log"
                },
                {
                    "name": "Slow Execution",
                    "condition": "execution_time > 30.0",
                    "severity": "warning",
                    "action": "log"
                },
                {
                    "name": "Convergence Failure",
                    "condition": "convergence_rate < 0.70",
                    "severity": "warning",
                    "action": "log"
                }
            ],
            "notification_channels": {
                "log_file": "production/logs/alerts.log",
                "metrics_file": "production/monitoring/alerts_history.json"
            }
        }
        
        alerts_dir = Path("production/monitoring")
        alerts_dir.mkdir(parents=True, exist_ok=True)
        
        with open(alerts_dir / "alerts_config.json", 'w') as f:
            json.dump(alerts_config, f, indent=2)
        
        logger.info("âœ“ Alerts configuration created")
        
        # Create alert handler script
        alert_script = """#!/usr/bin/env python3
import json
from pathlib import Path
from datetime import datetime

def trigger_alert(rule_name, severity, message):
    alert = {
        'timestamp': datetime.now().isoformat(),
        'rule': rule_name,
        'severity': severity,
        'message': message
    }
    
    # Log to file
    log_file = Path('production/logs/alerts.log')
    with open(log_file, 'a') as f:
        f.write(f'{json.dumps(alert)}\\n')
    
    # Save to history
    history_file = Path('production/monitoring/alerts_history.json')
    history = []
    if history_file.exists():
        with open(history_file) as f:
            history = json.load(f)
    
    history.append(alert)
    
    with open(history_file, 'w') as f:
        json.dump(history[-100:], f, indent=2)  # Keep last 100 alerts

if __name__ == '__main__':
    trigger_alert('Test Alert', 'info', 'Alert system initialized')
"""
        
        with open(alerts_dir / "alert_handler.py", 'w') as f:
            f.write(alert_script)
        
        logger.info("âœ“ Alert handler script created")
        
        # Test alert system
        success, output = self.run_command(
            "python production/monitoring/alert_handler.py",
            "Test Alert System"
        )
        
        self.metrics["alerts_configured"] = success
        logger.info("\nâœ… AUTOMATED ALERTS CONFIGURED")
        return True
    
    def generate_deployment_report(self):
        """Generate comprehensive deployment report"""
        logger.info("\n" + "="*80)
        logger.info("GENERATING DEPLOYMENT REPORT")
        logger.info("="*80)
        
        duration = (datetime.now() - self.start_time).total_seconds()
        
        report = f"""# Full Integration Deployment Report

**Date:** {datetime.now().strftime("%Y-%m-%d %H:%M:%S")}  
**Duration:** {duration:.2f} seconds  
**Status:** {'âœ… COMPLETE' if all(self.metrics.values()) else 'âš ï¸ PARTIAL'}

---

## Deployment Steps Summary

| Step | Status | Description |
|------|--------|-------------|
| 1 | {'âœ…' if self.metrics['staging_deployed'] else 'âŒ'} | Deploy to Staging Environment |
| 2 | {'âœ…' if self.metrics['integration_tests_passed'] else 'âŒ'} | Run Full Integration Tests |
| 3 | {'âœ…' if self.metrics['convergence_validated'] else 'âŒ'} | Validate Convergence Metrics |
| 4 | {'âœ…' if self.metrics['performance_benchmarked'] else 'âŒ'} | Performance Benchmarking |
| 5 | {'âœ…' if self.metrics['production_deployed'] else 'âŒ'} | Deploy to Production |
| 6 | {'âœ…' if self.metrics['monitoring_configured'] else 'âŒ'} | Set Up Continuous Monitoring |
| 7 | {'âœ…' if self.metrics['alerts_configured'] else 'âŒ'} | Configure Automated Alerts |

---

## Metrics Summary

```json
{json.dumps(self.metrics, indent=2)}
```

---

## Deployment Log

Total Actions: {len(self.deployment_log)}

"""
        
        for i, log_entry in enumerate(self.deployment_log[-20:], 1):  # Last 20 entries
            status = "âœ“" if log_entry["success"] else "âœ—"
            report += f"{i}. {status} {log_entry['description']} ({log_entry['timestamp']})\n"
        
        report += f"""

---

## Next Steps

### Immediate Actions
1. {'âœ…' if self.metrics['production_deployed'] else 'â³'} Verify production deployment
2. {'âœ…' if self.metrics['monitoring_configured'] else 'â³'} Start monitoring services
3. {'âœ…' if self.metrics['alerts_configured'] else 'â³'} Test alert notifications
4. â³ Schedule first production run

### Ongoing Maintenance
1. Monitor execution metrics daily
2. Review convergence trends weekly
3. Update configurations as needed
4. Maintain documentation

---

## Production Configuration

**Environment:** Production  
**Integration Mode:** Unified  
**Iterations:** 3  
**Agents:** Enabled  
**Convergence:** Enabled  
**Monitoring:** {'Enabled' if self.metrics['monitoring_configured'] else 'Disabled'}  
**Alerts:** {'Enabled' if self.metrics['alerts_configured'] else 'Disabled'}  

---

## Files Deployed

### Staging
- staging/GBOGEB_ABACUS_DOW_INTEGRATION_BRIDGE.py
- staging/UNIFIED_GLOB_CONFIG.yaml
- staging/test_integration_bridge.py
- staging/config/staging_config.json

### Production
- production/GBOGEB_ABACUS_DOW_INTEGRATION_BRIDGE.py
- production/UNIFIED_GLOB_CONFIG.yaml
- production/INTEGRATION_GUIDE.md
- production/config/production_config.json
- production/monitoring/monitoring_config.json
- production/monitoring/alerts_config.json
- production/monitoring/monitor.py
- production/monitoring/alert_handler.py

---

**Deployment Completed:** {datetime.now().strftime("%Y-%m-%d %H:%M:%S")}  
**Total Duration:** {duration:.2f} seconds  
**Overall Status:** {'âœ… SUCCESS' if all(self.metrics.values()) else 'âš ï¸ PARTIAL SUCCESS'}

---

*Generated by Full Integration Deployer v1.0.0*
"""
        
        report_file = Path("FULL_DEPLOYMENT_REPORT.md")
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write(report)
        
        logger.info(f"âœ“ Deployment report saved: {report_file}")
        
        return report
    
    def run_full_deployment(self):
        """Execute complete deployment pipeline"""
        logger.info("\n" + "="*80)
        logger.info("STARTING FULL INTEGRATION DEPLOYMENT")
        logger.info("="*80)
        
        steps = [
            ("Deploy to Staging", self.step_1_deploy_staging),
            ("Run Full Integration Tests", self.step_2_run_full_integration_tests),
            ("Validate Convergence", self.step_3_validate_convergence),
            ("Performance Benchmark", self.step_4_performance_benchmark),
            ("Deploy to Production", self.step_5_deploy_production),
            ("Setup Monitoring", self.step_6_setup_monitoring),
            ("Configure Alerts", self.step_7_configure_alerts)
        ]
        
        for step_name, step_func in steps:
            try:
                logger.info(f"\n{'='*80}")
                logger.info(f"Executing: {step_name}")
                logger.info(f"{'='*80}")
                
                success = step_func()
                
                if not success and step_name == "Deploy to Production":
                    logger.warning(f"âš ï¸ {step_name} skipped due to unmet prerequisites")
                elif not success:
                    logger.warning(f"âš ï¸ {step_name} completed with warnings")
                
            except Exception as e:
                logger.error(f"âŒ {step_name} failed: {e}", exc_info=True)
        
        # Generate final report
        self.generate_deployment_report()
        
        # Summary
        completed_steps = sum(1 for v in self.metrics.values() if v)
        total_steps = len(self.metrics)
        
        logger.info("\n" + "="*80)
        logger.info("DEPLOYMENT COMPLETE")
        logger.info(f"Steps Completed: {completed_steps}/{total_steps}")
        logger.info(f"Duration: {(datetime.now() - self.start_time).total_seconds():.2f}s")
        logger.info("="*80)
        
        return completed_steps == total_steps


def main():
    """Main execution"""
    deployer = FullIntegrationDeployer()
    success = deployer.run_full_deployment()
    
    if success:
        logger.info("\nâœ… FULL DEPLOYMENT SUCCESSFUL")
        return 0
    else:
        logger.warning("\nâš ï¸ DEPLOYMENT COMPLETED WITH WARNINGS")
        return 1


if __name__ == "__main__":
    sys.exit(main())
